# -*- coding: utf-8 -*-
"""v1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/saransh17/NLPA_SemanticSimilarity/blob/main/v1.ipynb
"""

import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
import nltk
from nltk.corpus import stopwords
import io
import re
from tqdm import tqdm
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
# import import_ipynb
# from custom_matching_layers import *
from sklearn.model_selection import train_test_split

tf.config.experimental.list_physical_devices()

cpus = tf.config.experimental.list_physical_devices(device_type='CPU')
tf.config.experimental.set_visible_devices(devices=cpus[0], device_type='CPU')
# tf.config.experimental.set_memory_growth(device=cpus[0], enable=True)

print(os.listdir("./datasets"))
print(os.listdir("./datasets/MSRP"))

df_qqp = pd.read_csv('./datasets/MSRP/msrp.csv', delimiter=',')

df_qqp

nltk.download('stopwords')
stops = set(stopwords.words('english'))

def text_to_word_list(text, remove_stop_words=False):
    ''' Pre process and convert texts to a list of words '''
    text = str(text)
    text = text.lower()

    # Clean the text
    text = re.sub(r"[^A-Za-z0-9^,!.\/'+-=]", " ", text)
    text = re.sub(r"what's", "what is ", text)
    text = re.sub(r"\'s", " ", text)
    text = re.sub(r"\'ve", " have ", text)
    text = re.sub(r"can't", "cannot ", text)
    text = re.sub(r"n't", " not ", text)
    text = re.sub(r"i'm", "i am ", text)
    text = re.sub(r"\'re", " are ", text)
    text = re.sub(r"\'d", " would ", text)
    text = re.sub(r"\'ll", " will ", text)
    text = re.sub(r",", " ", text)
    text = re.sub(r"\.", " ", text)
    text = re.sub(r"!", " ! ", text)
    text = re.sub(r"\/", " ", text)
    text = re.sub(r"\^", " ^ ", text)
    text = re.sub(r"\+", " + ", text)
    text = re.sub(r"\-", " - ", text)
    text = re.sub(r"\=", " = ", text)
    text = re.sub(r"'", " ", text)
    text = re.sub(r"(\d+)(k)", r"\g<1>000", text)
    text = re.sub(r":", " : ", text)
    text = re.sub(r" e g ", " eg ", text)
    text = re.sub(r" b g ", " bg ", text)
    text = re.sub(r" u s ", " american ", text)
    text = re.sub(r"\0s", "0", text)
    text = re.sub(r" 9 11 ", "911", text)
    text = re.sub(r"e - mail", "email", text)
    text = re.sub(r"j k", "jk", text)
    text = re.sub(r"\s{2,}", " ", text)

    text = text.split()
    
    if remove_stop_words:
        text = [wd for wd in text if wd not in stops]

    return text

text_to_word_list('step by step')

df_qqp['question1'] = df_qqp['question1'].map(lambda x: text_to_word_list(x))
df_qqp['question2'] = df_qqp['question2'].map(lambda x: text_to_word_list(x))
df_qqp['len_q1'] = df_qqp['question1'].map(lambda x: len(x))
df_qqp['len_q2'] = df_qqp['question2'].map(lambda x: len(x))

df_qqp.head()

df_qqp.describe()

sentence_len = 30
max_sentence_length = sentence_len
embeddings_len = 300
max_unique_words = 20000

temp = df_qqp[(df_qqp['len_q1']<sentence_len) & (df_qqp['len_q2']<sentence_len)]
temp

all_questions = list(temp['question1'].values) + list(temp['question2'].values)

# glove_embeddings = {}
# with open('8/model.txt') as f:
#     for line in f:
#         word, coefs = line.split(maxsplit=1)
#         coefs = np.fromstring(coefs, "f", sep=" ")
#         glove_embeddings[word.lower()] = coefs

# print("Found %s word vectors." % len(glove_embeddings))

glove_embeddings = {}
with open('glove.6B/glove.6B.300d.txt') as f:
    for line in f:
        word, coefs = line.split(maxsplit=1)
        coefs = np.fromstring(coefs, "f", sep=" ")
        glove_embeddings[word.lower()] = coefs

print("Found %s word vectors." % len(glove_embeddings))

len(glove_embeddings.keys())

# temp = temp[:4000]

word_to_index = dict()
index_to_word = ['<unk>']
questions_cols = ['question1', 'question2']
q1_indexes = []
q2_indexes = []
hits = 0
misses = 0
for index, row in tqdm(temp.iterrows()):
    for question in questions_cols:
        q2n = []
        for word in row[question]:
            if word not in glove_embeddings:
                misses+=1
#                 print(word,"here1")
                continue
            hits+=1
            if word not in word_to_index:
#                 print(word,"here2")
                word_to_index[word] = len(index_to_word)
                q2n.append(len(index_to_word))
                index_to_word.append(word)
            else:
#                 print(word,"here3")
                q2n.append(word_to_index[word])
#         print(q2n)
#         print(row[question])
#         print(row)
#         break
        if question == 'question1':
            q1_indexes.append(q2n)
        else:
            q2_indexes.append(q2n)
#     if index>4:
#         break
print(hits,misses)
temp['q1_indexes'] = q1_indexes
temp['q2_indexes'] = q2_indexes

temp

len(word_to_index)

num_tokens = len(word_to_index) + 2
embedding_matrix = np.zeros((num_tokens, embeddings_len))
for word, i in word_to_index.items():
    embedding_matrix[i] = glove_embeddings[word]

temp = temp[temp['q1_indexes'].map(lambda x:len(x))<max_sentence_length]
temp = temp[temp['q2_indexes'].map(lambda x:len(x))<max_sentence_length]
temp.loc[:,'l'] = pd.Series(list(keras.preprocessing.sequence.pad_sequences(temp['q1_indexes'], maxlen=max_sentence_length)),index=temp.index)
temp.loc[:,'r'] = pd.Series(list(keras.preprocessing.sequence.pad_sequences(temp['q2_indexes'], maxlen=max_sentence_length)),index=temp.index)

temp

# temp = temp[:10000]

X_l = np.stack(temp['l'].to_numpy(),axis=1).T
X_r = np.stack(temp['r'].to_numpy(),axis=1).T
X = np.stack([X_l,X_r],axis=1)
# X = np.stack(temp['l'].to_numpy(),axis=1).T
Y = np.stack(temp['is_duplicate'].to_numpy())
X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=0.2)

"""## Building Model"""

embedding_layer = layers.Embedding(
    num_tokens,
    embeddings_len,
    embeddings_initializer=keras.initializers.Constant(embedding_matrix),
    trainable=False,
)

left_input = layers.Input(shape=(max_sentence_length,), dtype='int32')
right_input = layers.Input(shape=(max_sentence_length,), dtype='int32')

encoded_left = embedding_layer(left_input)
encoded_right = embedding_layer(right_input)

# shared_bilstm = layers.Bidirectional(layers.LSTM(20, return_sequences=True))
forward_layer = layers.LSTM(20,dropout=0.80, return_sequences=True, go_backwards=True)
backward_layer = layers.LSTM(20,dropout=0.80, return_sequences=True, go_backwards=True)

forward_left = forward_layer(encoded_left)
forward_right = forward_layer(encoded_right)
backward_left = backward_layer(encoded_left)
backward_right = backward_layer(encoded_right)

perspectives = 10

# class matching_1_backward(layers.Layer):
#     def __init__(self, units=10):
#         super(matching_1_backward, self).__init__()
#         self.units = units
    
#     def build(self, input_shape):
#         self.w = self.add_weight(
#             shape=(input_shape[0][-1], self.units),
#             initializer="random_normal",
#             trainable=True,
#         )

#     def call(self, inputs, **kwargs):
#         left = tf.matmul(inputs[0],self.w)
#         right = tf.matmul(inputs[1][:,0],self.w)
#         m = layers.Dot(axes=(2, 1))([left,right])
#         return m
    
class matching_1_backward(layers.Layer):
    def __init__(self, units=7):
        super(matching_1_backward, self).__init__()
        self.units = units
    
    def build(self, input_shape):
        self.w = self.add_weight(
            shape=(input_shape[0][-1], self.units),
            initializer="random_normal",
            trainable=True,
        )

    @tf.function
    def call(self, inputs, **kwargs):
        left = tf.matmul(inputs[0],self.w)
        right = tf.matmul(inputs[1][:,0],self.w)
        ms = []
        for i,p in enumerate(range(self.units)):
            custom_shape = list(left.shape)
            custom_shape[-1] = 1
            custom_shape[0] = -1
#             print(custom_shape)
            left_p=tf.reshape(left[:,:,i],custom_shape)
            custom_shape = list(right.shape)
            custom_shape[0] = -1
            custom_shape[-1] = 1
            right_p=tf.reshape(right[:,i],custom_shape)
            kk = layers.Dot(axes=(2, 1))([left_p,right_p])
            ms.append(kk)
        m = tf.stack(ms,axis=2)
        return m
    
    def compute_output_shape(self):
        return (self.w.shape[0])

class matching_1_forward(layers.Layer):
    def __init__(self, units=7):
        super(matching_1_forward, self).__init__()
        self.units = units
    
    def build(self, input_shape):
        self.w = self.add_weight(
            shape=(input_shape[0][-1], self.units),
            initializer="random_normal",
            trainable=True,
        )

    @tf.function
    def call(self, inputs, **kwargs):
        left = tf.matmul(inputs[0],self.w)
        right = tf.matmul(inputs[1][:,-1],self.w)
        ms = []
        for i,p in enumerate(range(self.units)):
            custom_shape = list(left.shape)
            custom_shape[0] = -1
            custom_shape[-1] = 1
#             print(custom_shape)
            left_p=tf.reshape(left[:,:,i],custom_shape)
            custom_shape = list(right.shape)
            custom_shape[0] = -1
            custom_shape[-1] = 1
            right_p=tf.reshape(right[:,i],custom_shape)
            kk = layers.Dot(axes=(2, 1))([left_p,right_p])
            ms.append(kk)
        m = tf.stack(ms,axis=2)
        return m
    
    def compute_output_shape(self):
        return (self.w.shape[0])

match_1_forward_pq = matching_1_forward()([forward_left,forward_right])
match_1_forward_qp = matching_1_forward()([forward_right,forward_left])
match_1_backward_pq = matching_1_backward()([backward_left,backward_right])
match_1_backward_qp = matching_1_backward()([backward_right,backward_left])

all_matching_p = layers.Concatenate(axis=1)([match_1_forward_pq,match_1_forward_pq])
all_matching_q = layers.Concatenate(axis=1)([match_1_forward_qp,match_1_forward_qp])
aggregate = layers.Bidirectional(layers.LSTM(16))
aggregate_p = aggregate(all_matching_p)
aggregate_q = aggregate(all_matching_q)
concated = layers.Concatenate(axis=1)([aggregate_p,aggregate_q])
x = layers.Dense(32)(concated)
x = layers.Dense(1, activation='sigmoid')(x)

all_matching_p

model = keras.Model(inputs=[left_input,right_input], outputs=x, name="temp_model")
model.summary()

keras.utils.plot_model(model, "my_first_model_with_shape_info.png", show_shapes=True)

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

hist = model.fit([X_train[:,0], X_train[:,1]], Y_train, batch_size=10, epochs=10,
                            validation_data=([X_validation[:,0], X_validation[:,1]], Y_validation))

# Plot accuracy
print(hist.history)
plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plot loss
plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper right')
plt.show()

X_train[:,0].shape

